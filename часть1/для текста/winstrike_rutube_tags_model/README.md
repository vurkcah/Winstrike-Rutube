---
base_model: DeepPavlov/rubert-base-cased-sentence
library_name: sentence-transformers
pipeline_tag: sentence-similarity
tags:
- sentence-transformers
- sentence-similarity
- feature-extraction
- generated_from_trainer
- dataset_size:4720
- loss:CosineSimilarityLoss
widget:
- source_sentence: '–ü–æ–¥ –¥—Ä—É–≥–∏–º —É–≥–ª–æ–º | –í—ã–ø—É—Å–∫ 3 | –ê—Å—Ç—Ä–æ–ª–æ–≥–∏—è –î–∞–≤–∞–π—Ç–µ –ø–æ–≥–∞–¥–∞–µ–º, –Ω–∞
    —á—Ç–æ –∂–µ –≤ –Ω–æ–≤–æ–º –≤—ã–ø—É—Å–∫–µ "–ü–æ–¥ –¥—Ä—É–≥–∏–º —É–≥–ª–æ–º" –º—ã –ø–æ—Å–º–æ—Ç—Ä–∏–º. –ù–∞ –∞—Å—Ç—Ä–æ‚Ä¶ -–Ω–æ–º–∏—é?! –ù–µ—Ç!
    –ê—Å—Ç—Ä–æ‚Ä¶ -—Ñ–∏–∑–∏–∫—É?! –ù–µ–µ–µ–µ! –¶–≤–µ—Ç—ã –∞—Å—Ç—Ä—ã?! –î–∞ –Ω–µ—Ç! –ê—Å—Ç—Ä–æ–ª–æ–≥–∏—é! –ú—ã —Ä–∞–∑–±–µ—Ä—ë–º—Å—è –≤ –µ—ë –ø—Ä–∏–Ω—Ü–∏–ø–∞—Ö.
    –ü–æ–π–º—ë–º, –∫–∞–∫–æ–µ —Å–æ–∑–≤–µ–∑–¥–∏–µ –∏–∑ –∑–æ–¥–∏–∞–∫–∞–ª—å–Ω–æ–≥–æ –∫—Ä—É–≥–∞ –ø–æ—Ç–µ—Ä—è–ª–æ—Å—å. –ß—Ç–æ —Ç–∞–∫–æ–µ –º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ
    –∫–∞—Ä—Ç—ã? –í—ã—è—Å–Ω–∏–º –µ—Å—Ç—å –ª–∏ —Ç—É—Ç –Ω–∞—É–∫–∞ –∏–ª–∏ –Ω–µ—Ç. –ö–∞–∫ —Å–≤—è–∑–∞–Ω—ã –∞—Å—Ç—Ä–æ–ª–æ–≥–∏—è –∏ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—è?
    –ò –ø—Ä–∏–º–µ–º —Ä–µ—à–µ–Ω–∏–µ, –≤–µ—Ä–∏—Ç—å –ª–∏ –≥–æ—Ä–æ—Å–∫–æ–ø–∞–º (–°–ø–æ–π–ª–µ—Ä: —ç—Ç–æ —Ä–µ—à–µ–Ω–∏–µ –ø–æ–≤–ª–∏—è–µ—Ç –Ω–∞ –≤–∞—à—É
    —Å—É–¥—å–±—É).'
  sentences:
  - '–ü—É—Ç–µ—à–µ—Å—Ç–≤–∏—è: –¢–∏–ø –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è: –ë—é–¥–∂–µ—Ç–Ω—ã–π –æ—Ç–¥—ã—Ö'
  - '–ò–∑–æ–±—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–æ: –°–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–æ'
  - '–°–æ–±—ã—Ç–∏—è –∏ –¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: –ë–∏–∑–Ω–µ—Å-–≤—ã—Å—Ç–∞–≤–∫–∏ –∏ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏'
- source_sentence: –ê—Ä—Ç–º–µ—Ö–∞–Ω–∏–∫–∞. –ò–Ω—Ç–µ—Ä–≤—å—é. –î–µ–Ω–∏—Å –†–æ–≥–æ–≤. –ß—Ç–æ —Ç–∞–∫–æ–µ –º–µ—Ç–∞–≤—Å–µ–ª–µ–Ω–Ω–∞—è? –ì–¥–µ
    –º—ã –º–æ–∂–µ–º –æ—â—É—Ç–∏—Ç—å –∫–∞–∫ –æ–Ω–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —É–∂–µ —Å–µ–≥–æ–¥–Ω—è? –ö—Ç–æ —Å–æ–∑–¥–∞–µ—Ç –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏?
  sentences:
  - '–ú—É–∑—ã–∫–∞ –∏ –∞—É–¥–∏–æ: –†–æ–∫ –º—É–∑—ã–∫–∞: –õ–µ–≥–∫–∏–π —Ä–æ–∫'
  - –ö–∞—Ä—å–µ—Ä–∞
  - '–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ: –û–±—É—á–µ–Ω–∏–µ –¥–µ—Ç–µ–π —Å –æ—Å–æ–±—ã–º–∏ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—è–º–∏'
- source_sentence: –ê—Ä—Ç–º–µ—Ö–∞–Ω–∏–∫–∞. –ö–æ–Ω—Ü–µ—Ä—Ç –≥—Ä—É–ø–ø—ã –î–∏–∫—Ç–æ—Ñ–æ–Ω. –ö–æ–Ω—Ü–µ—Ä—Ç –≥—Ä—É–ø–ø—ã –î–∏–∫—Ç–æ—Ñ–æ–Ω.
  sentences:
  - '–°–æ–±—ã—Ç–∏—è –∏ –¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: –°–ø–æ—Ä—Ç–∏–≤–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è'
  - '–ú–∞—Å—Å–æ–≤–∞—è –∫—É–ª—å—Ç—É—Ä–∞: –Æ–º–æ—Ä –∏ —Å–∞—Ç–∏—Ä–∞'
  - '–°–æ–±—ã—Ç–∏—è –∏ –¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: –ö–æ–Ω—Ü–µ—Ä—Ç—ã –∏ –º—É–∑—ã–∫–∞–ª—å–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è'
- source_sentence: –û–¥–∏–Ω –≤—ã—Ö–æ–¥–Ω–æ–π | –í—ã–ø—É—Å–∫ 21 | –°–ø–æ—Ä—Ç –°–µ–≥–æ–¥–Ω—è –¢–∞—à—ç –∑–∞–π–º–µ—Ç—Å—è —Å–ø–æ—Ä—Ç–æ–º
    –∏ —Ä–∞—Å—Å–∫–∞–∂–µ—Ç –≤–∞–º, –≥–¥–µ –º–æ–∂–Ω–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Å—Ç–∏ —Å–≤–æ–π –≤—ã—Ö–æ–¥–Ω–æ–π, –∞ –µ—â–µ –∏ –ø–æ–∫–∞—á–∞—Ç—å—Å—è
    –∫ –ª–µ—Ç–Ω–µ–º—É —Å–µ–∑–æ–Ω—É, –∫–∞–∫ –≤—Å–µ–≥–¥–∞ –∑–∞ –¥–µ–Ω—å–≥–∏ –∏ –±–µ—Å–ø–ª–∞—Ç–Ω–æ.
  sentences:
  - '–°–æ–±—ã—Ç–∏—è –∏ –¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: –ü–∞—Ä–∫–∏ —Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏–π –∏ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∫–∏'
  - '–ë–∏–∑–Ω–µ—Å –∏ —Ñ–∏–Ω–∞–Ω—Å—ã: –ü—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç—å –∏ —Å—Ñ–µ—Ä–∞ —É—Å–ª—É–≥: –®–≤–µ–π–Ω–∞—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç—å'
  - '–ú–∞—Å—Å–æ–≤–∞—è –∫—É–ª—å—Ç—É—Ä–∞: –°–º–µ—Ä—Ç–∏ –∑–Ω–∞–º–µ–Ω–∏—Ç–æ—Å—Ç–µ–π'
- source_sentence: ¬´–ú—É–∑–õ–æ—Ñ—Ç - –ü–æ–¥–∫–∞—Å—Ç¬ª —Å –ú–∞—Ä—Ç–∏—Ä–æ—Å—è–Ω–æ–º, –°–æ—Ä–æ–∫–∏–Ω—ã–º, –ê–≤–µ—Ä–∏–Ω—ã–º, –ú–∞—Ç—É–∞
    - 3 –∏—é–Ω—è –≤ 15:00! –ü–æ–¥–ø–∏—Å—ã–≤–∞–π—Å—è, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å!
  sentences:
  - '–ú–µ–¥–∏—Ü–∏–Ω–∞: –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è: –≠–Ω–¥–æ–∫—Ä–∏–Ω–æ–ª–æ–≥–∏—è'
  - '–ú–∞—Å—Å–æ–≤–∞—è –∫—É–ª—å—Ç—É—Ä–∞: –Æ–º–æ—Ä –∏ —Å–∞—Ç–∏—Ä–∞'
  - '–°–æ–±—ã—Ç–∏—è –∏ –¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: –ö–æ–Ω—Ü–µ—Ä—Ç—ã –∏ –º—É–∑—ã–∫–∞–ª—å–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è'
---

# SentenceTransformer based on DeepPavlov/rubert-base-cased-sentence

This is a [sentence-transformers](https://www.SBERT.net) model finetuned from [DeepPavlov/rubert-base-cased-sentence](https://huggingface.co/DeepPavlov/rubert-base-cased-sentence). It maps sentences & paragraphs to a 768-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.

## Model Details

### Model Description
- **Model Type:** Sentence Transformer
- **Base model:** [DeepPavlov/rubert-base-cased-sentence](https://huggingface.co/DeepPavlov/rubert-base-cased-sentence) <!-- at revision 78b5122d6365337dd4114281b0d08cd1edbb3bc8 -->
- **Maximum Sequence Length:** 512 tokens
- **Output Dimensionality:** 768 tokens
- **Similarity Function:** Cosine Similarity
<!-- - **Training Dataset:** Unknown -->
<!-- - **Language:** Unknown -->
<!-- - **License:** Unknown -->

### Model Sources

- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)
- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)
- **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers)

### Full Model Architecture

```
SentenceTransformer(
  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
)
```

## Usage

### Direct Usage (Sentence Transformers)

First install the Sentence Transformers library:

```bash
pip install -U sentence-transformers
```

Then you can load this model and run inference.
```python
from sentence_transformers import SentenceTransformer

# Download from the ü§ó Hub
model = SentenceTransformer("sentence_transformers_model_id")
# Run inference
sentences = [
    '¬´–ú—É–∑–õ–æ—Ñ—Ç - –ü–æ–¥–∫–∞—Å—Ç¬ª —Å –ú–∞—Ä—Ç–∏—Ä–æ—Å—è–Ω–æ–º, –°–æ—Ä–æ–∫–∏–Ω—ã–º, –ê–≤–µ—Ä–∏–Ω—ã–º, –ú–∞—Ç—É–∞ - 3 –∏—é–Ω—è –≤ 15:00! –ü–æ–¥–ø–∏—Å—ã–≤–∞–π—Å—è, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å!',
    '–°–æ–±—ã—Ç–∏—è –∏ –¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: –ö–æ–Ω—Ü–µ—Ä—Ç—ã –∏ –º—É–∑—ã–∫–∞–ª—å–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è',
    '–ú–µ–¥–∏—Ü–∏–Ω–∞: –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è: –≠–Ω–¥–æ–∫—Ä–∏–Ω–æ–ª–æ–≥–∏—è',
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 768]

# Get the similarity scores for the embeddings
similarities = model.similarity(embeddings, embeddings)
print(similarities.shape)
# [3, 3]
```

<!--
### Direct Usage (Transformers)

<details><summary>Click to see the direct usage in Transformers</summary>

</details>
-->

<!--
### Downstream Usage (Sentence Transformers)

You can finetune this model on your own dataset.

<details><summary>Click to expand</summary>

</details>
-->

<!--
### Out-of-Scope Use

*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->

<!--
## Bias, Risks and Limitations

*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->

<!--
### Recommendations

*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->

## Training Details

### Training Dataset

#### Unnamed Dataset


* Size: 4,720 training samples
* Columns: <code>sentence_0</code>, <code>sentence_1</code>, and <code>label</code>
* Approximate statistics based on the first 1000 samples:
  |         | sentence_0                                                                          | sentence_1                                                                       | label                                                            |
  |:--------|:------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|:-----------------------------------------------------------------|
  | type    | string                                                                              | string                                                                           | float                                                            |
  | details | <ul><li>min: 14 tokens</li><li>mean: 89.78 tokens</li><li>max: 512 tokens</li></ul> | <ul><li>min: 2 tokens</li><li>mean: 8.85 tokens</li><li>max: 19 tokens</li></ul> | <ul><li>min: 0.05</li><li>mean: 0.38</li><li>max: 0.99</li></ul> |
* Samples:
  | sentence_0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | sentence_1                                                    | label             |
  |:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------|:------------------|
  | <code>–ú–ê–ö–°–ò–ú –ù–ê–†–û–î–ù–´–ô –í—ã–ø—É—Å–∫ ‚Ññ13 –ì–û–¢–û–í–ò–ú –ß–ê–®–£–®–£–õ–ò –ü–û-–ì–†–£–ó–ò–ù–°–ö–ò –ü—Ä–µ–¥–ª–∞–≥–∞—é –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º –ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å —á–∞—à—É—à—É–ª–∏ –ø–æ-–≥—Ä—É–∑–∏–Ω—Å–∫–∏.</code>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | <code>–ë–∏–∑–Ω–µ—Å –∏ —Ñ–∏–Ω–∞–Ω—Å—ã: –≠–∫–æ–Ω–æ–º–∏–∫–∞: –¶–µ–Ω—ã –Ω–∞ –±–µ–Ω–∑–∏–Ω</code>      | <code>0.1</code>  |
  | <code>¬´–ú—É–∑–õ–æ—Ñ—Ç-–ü–æ–¥–∫–∞—Å—Ç¬ª —Å –°–∞—Ä—É—Ö–∞–Ω–æ–≤—ã–º ‚Äî 20 –Ω–æ—è–±—Ä—è –≤ 15:00! –ü–æ–¥–ø–∏—Å—ã–≤–∞–π—Å—è, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å!</code>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | <code>–ú–∞—Å—Å–æ–≤–∞—è –∫—É–ª—å—Ç—É—Ä–∞</code>                                | <code>0.85</code> |
  | <code>–ö–æ–º–∞–Ω–¥–∞ "3/21" –≤ –ì—Ä–∞–Ω–¥–¢—É—Ä–µ-2022: —Å–µ–ª–æ –¢–∞–≥–∞—Ä—Ö–∞–π –ö–æ–º–∞–Ω–¥–∞ "3/21" –ø—Ä–∏–µ—Ö–∞–ª–∞ –≤ —Å–µ–ª–æ –¢–∞–≥–∞—Ä—Ö–∞–π, –Ω–∞ —Ä–æ–¥–∏–Ω—É –ì–µ—Ä–æ—è –°–æ–≤–µ—Ç—Å–∫–æ–≥–æ –°–æ—é–∑–∞, –ª–µ–≥–µ–Ω–¥–∞—Ä–Ω–æ–≥–æ –±—É—Ä—è—Ç—Å–∫–æ–≥–æ —Å–Ω–∞–π–ø–µ—Ä–∞ –®–∞–º–±—ã–ª–∞ –ï—à–µ–µ–≤–∏—á–∞ –¢—É–ª–∞–µ–≤–∞. –ò–∑ –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –±—É—Ä—è—Ç—Å–∫–æ–≥–æ —Å–µ–ª–∞ –Ω–∞ –í–µ–ª–∏–∫—É—é –û—Ç–µ—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –≤–æ–π–Ω—É —É—à–ª–∏ 120 —á–µ–ª–æ–≤–µ–∫. 55 —Ç–∞–≥–∞—Ä—Ö–∞–π—Ü–µ–≤ –≤–µ—Ä–Ω—É–ª–∏—Å—å –ø–æ—Å–ª–µ –≤–æ–π–Ω—ã –∫ —Ä–æ–¥–Ω—ã–º.  –ê–∫–∫–∞—É–Ω—Ç –∫–æ–º–∞–Ω–¥—ã –≤ Rutube - / ‚Ä¶‚Ä¶‚Ä¶‚Ä¶ –ì—Ä–∞–Ω–¥–¢—É—Ä ¬´–ë–∞–π–∫–∞–ª—å—Å–∫–∞—è –º–∏–ª—è 2022. –ú–µ—Å—Ç–∞ —Å–∏–ª—ã¬ª - —ç—Ç–æ –≥—Ä–∞–Ω–¥–∏–æ–∑–Ω–æ–µ –∞–≤—Ç–æ–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ, —Å–æ—Å—Ç–æ—è–≤—à–µ–µ—Å—è —Å 20 —Ñ–µ–≤—Ä–∞–ª—è –ø–æ 9 –º–∞—Ä—Ç–∞. –í —ç—Ç–æ–º –ø—Ä–æ–±–µ–≥–µ –ø—Ä–∏–Ω—è–ª–∏ —É—á–∞—Å—Ç–∏–µ 14 –∫–æ–º–∞–Ω–¥, —Å—Ç–∞—Ä—Ç–æ–≤–∞–≤—à–∏—Ö –∏–∑ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–∞, –ú–æ—Å–∫–≤—ã, –ö–∞–∑–∞–Ω–∏ –∏ –í–ª–∞–¥–∏–≤–æ—Å—Ç–æ–∫–∞ –∏ —Ñ–∏–Ω–∏—à–∏—Ä–æ–≤–∞–≤—à–∏—Ö –≤ —Å–µ–ª–µ –ú–∞–∫—Å–∏–º–∏—Ö–∞, –º–µ—Å—Ç–µ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –§–µ—Å—Ç–∏–≤–∞–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ ¬´–ë–∞–π–∫–∞–ª—å—Å–∫–∞—è –º–∏–ª—è¬ª. –ö–∞–∂–¥–∞—è –∫–æ–º–∞–Ω–¥–∞ –ø–æ–ª—É—á–∏–ª–∞ —Å–≤–æ–π —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç –∏ –ø–æ 6 –∑–∞–¥–∞–Ω–∏–π, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã—Ö –Ω–∞ —Ç–µ–º—ã: ¬´–ì–µ–æ–≥—Ä–∞—Ñ–∏—è¬ª, ¬´–ü–∞—Ç—Ä–∏–æ—Ç–∏–∫–∞¬ª, ¬´–≠–∫–æ–ª–æ–≥–∏—è¬ª, ¬´–û–±—â–µ—Å—Ç–≤–æ¬ª, ¬´–ú–æ—è ¬´–ë–∞–π–∫–∞–ª—å—Å–∫–∞—è –º–∏–ª—è¬ª –∏ ¬´–°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–µ –∑–∞–¥–∞–Ω–∏–µ¬ª, –≤ —Ä–∞–º–∫–∞—Ö –∫–æ—Ç–æ—Ä–æ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–∏ –º–æ–≥–ª–∏ —Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å —Å–≤–æ–∏–º –∑—Ä–∏—Ç–µ–ª—è–º –æ —Ç–æ–º, —á—Ç–æ —Å—á–∏—Ç–∞—é—Ç –Ω—É–∂–Ω—ã–º –∏ –≤–∞–∂–Ω—ã–º. #baikalmile2022 #–ì—Ä–∞–Ω–¥–¢—É—Ä–ë–∞–π–∫–∞–ª—å—Å–∫–∞—è–ú–∏–ª—è2022 #–º–µ—Å—Ç–∞—Å–∏–ª—ã #—ç–Ω—Ç—É–∑–∏–∞—Å—Ç—ã—Å–∫–æ—Ä–æ—Å—Ç–∏ #–≥–∫—Ñ—Å–∫ #–±–∞–π–∫–∞–ª—å—Å–∫–∞—è–º–∏–ª—è #baikalmile #–≤–æ–¥–æ—Ö–æ–¥—ä #rutube #–º–æ—Ç–æ–≤–µ—Å–Ω–∞ #finecustommechanics #fire_center #–†–ì–û #–±—É—Ä—è—Ç–∏—è–±–æ–ª—å—à–µ—á–µ–º–±–∞–π–∫–∞–ª #–ú—É–∑–µ–π–ü–æ–±–µ–¥—ã #—Ä–∫—Å–ø–æ—Ä—Ç</code> | <code>–°–æ–±—ã—Ç–∏—è –∏ –¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: –¢–æ—Ä–≥–æ–≤—ã–µ —Ü–µ–Ω—Ç—Ä—ã</code> | <code>0.15</code> |
* Loss: [<code>CosineSimilarityLoss</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#cosinesimilarityloss) with these parameters:
  ```json
  {
      "loss_fct": "torch.nn.modules.loss.MSELoss"
  }
  ```

### Training Hyperparameters
#### Non-Default Hyperparameters

- `per_device_train_batch_size`: 16
- `per_device_eval_batch_size`: 16
- `num_train_epochs`: 10
- `multi_dataset_batch_sampler`: round_robin

#### All Hyperparameters
<details><summary>Click to expand</summary>

- `overwrite_output_dir`: False
- `do_predict`: False
- `eval_strategy`: no
- `prediction_loss_only`: True
- `per_device_train_batch_size`: 16
- `per_device_eval_batch_size`: 16
- `per_gpu_train_batch_size`: None
- `per_gpu_eval_batch_size`: None
- `gradient_accumulation_steps`: 1
- `eval_accumulation_steps`: None
- `torch_empty_cache_steps`: None
- `learning_rate`: 5e-05
- `weight_decay`: 0.0
- `adam_beta1`: 0.9
- `adam_beta2`: 0.999
- `adam_epsilon`: 1e-08
- `max_grad_norm`: 1
- `num_train_epochs`: 10
- `max_steps`: -1
- `lr_scheduler_type`: linear
- `lr_scheduler_kwargs`: {}
- `warmup_ratio`: 0.0
- `warmup_steps`: 0
- `log_level`: passive
- `log_level_replica`: warning
- `log_on_each_node`: True
- `logging_nan_inf_filter`: True
- `save_safetensors`: True
- `save_on_each_node`: False
- `save_only_model`: False
- `restore_callback_states_from_checkpoint`: False
- `no_cuda`: False
- `use_cpu`: False
- `use_mps_device`: False
- `seed`: 42
- `data_seed`: None
- `jit_mode_eval`: False
- `use_ipex`: False
- `bf16`: False
- `fp16`: False
- `fp16_opt_level`: O1
- `half_precision_backend`: auto
- `bf16_full_eval`: False
- `fp16_full_eval`: False
- `tf32`: None
- `local_rank`: 0
- `ddp_backend`: None
- `tpu_num_cores`: None
- `tpu_metrics_debug`: False
- `debug`: []
- `dataloader_drop_last`: False
- `dataloader_num_workers`: 0
- `dataloader_prefetch_factor`: None
- `past_index`: -1
- `disable_tqdm`: False
- `remove_unused_columns`: True
- `label_names`: None
- `load_best_model_at_end`: False
- `ignore_data_skip`: False
- `fsdp`: []
- `fsdp_min_num_params`: 0
- `fsdp_config`: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}
- `fsdp_transformer_layer_cls_to_wrap`: None
- `accelerator_config`: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}
- `deepspeed`: None
- `label_smoothing_factor`: 0.0
- `optim`: adamw_torch
- `optim_args`: None
- `adafactor`: False
- `group_by_length`: False
- `length_column_name`: length
- `ddp_find_unused_parameters`: None
- `ddp_bucket_cap_mb`: None
- `ddp_broadcast_buffers`: False
- `dataloader_pin_memory`: True
- `dataloader_persistent_workers`: False
- `skip_memory_metrics`: True
- `use_legacy_prediction_loop`: False
- `push_to_hub`: False
- `resume_from_checkpoint`: None
- `hub_model_id`: None
- `hub_strategy`: every_save
- `hub_private_repo`: False
- `hub_always_push`: False
- `gradient_checkpointing`: False
- `gradient_checkpointing_kwargs`: None
- `include_inputs_for_metrics`: False
- `eval_do_concat_batches`: True
- `fp16_backend`: auto
- `push_to_hub_model_id`: None
- `push_to_hub_organization`: None
- `mp_parameters`: 
- `auto_find_batch_size`: False
- `full_determinism`: False
- `torchdynamo`: None
- `ray_scope`: last
- `ddp_timeout`: 1800
- `torch_compile`: False
- `torch_compile_backend`: None
- `torch_compile_mode`: None
- `dispatch_batches`: None
- `split_batches`: None
- `include_tokens_per_second`: False
- `include_num_input_tokens_seen`: False
- `neftune_noise_alpha`: None
- `optim_target_modules`: None
- `batch_eval_metrics`: False
- `eval_on_start`: False
- `eval_use_gather_object`: False
- `batch_sampler`: batch_sampler
- `multi_dataset_batch_sampler`: round_robin

</details>

### Training Logs
| Epoch  | Step | Training Loss |
|:------:|:----:|:-------------:|
| 1.6949 | 500  | 0.1104        |
| 3.3898 | 1000 | 0.0502        |
| 5.0847 | 1500 | 0.0313        |
| 6.7797 | 2000 | 0.0234        |
| 8.4746 | 2500 | 0.0168        |


### Framework Versions
- Python: 3.10.12
- Sentence Transformers: 3.1.1
- Transformers: 4.44.2
- PyTorch: 2.4.1+cu121
- Accelerate: 0.34.2
- Datasets: 3.0.1
- Tokenizers: 0.19.1

## Citation

### BibTeX

#### Sentence Transformers
```bibtex
@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}
```

<!--
## Glossary

*Clearly define terms in order to be accessible across audiences.*
-->

<!--
## Model Card Authors

*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->

<!--
## Model Card Contact

*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->